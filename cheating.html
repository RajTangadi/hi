<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <link rel="stylesheet" href="cheating.css" />
  </head>
  <body>
    <div class="container">
      <table id="customers">
        <tr>
          <th>Aim</th>
          <th>Button</th>
        </tr>
        <tr>
          <td>
            <div>Linear Regression</div>
          </td>
          <td><button id="lexBtn">copy me</button></td>
        </tr>

        <tr>
          <td>
            <div>Logistic Regression</div>
          </td>
          <td><button id="yacBtn">copy me</button></td>
        </tr>

        <tr>
          <td>
            <div>Random Forest</div>
          </td>
          <td><button id="firstFollow">copy me</button></td>
        </tr>

        <tr>
          <td>
            <div>Decision Tree</div>
          </td>
          <td><button id="precedent">copy me</button></td>
        </tr>

        <tr>
          <td>
            <div>DBSCAN</div>
          </td>
          <td><button id="assembler">copy me</button></td>
        </tr>
      </table>

      <span id="span"></span>

      <div class="div">
        <textarea id="lex">
            import pandas as pd
            from sklearn.model_selection import train_test_split
            from sklearn.linear_model import LinearRegression
            from sklearn.metrics import mean_squared_error, r2_score
            import matplotlib.pyplot as plt
            
            # Load the dataset
            data = pd.read_csv('/content/housing - housing.csv')  # Replace 'your_dataset.csv' with the actual file path
            
            # Data preprocessing
            # Remove rows with missing values
            data = data.dropna()
            
            # Select features and target variable
            X = data[['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']]
            y = data['median_house_value']
            
            # Split the data into training and testing sets
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Train the linear regression model
            regression_model = LinearRegression()
            regression_model.fit(X_train, y_train)
            
            # Make predictions on the test set
            y_pred = regression_model.predict(X_test)
            
            # Evaluate the model
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            print("Mean Squared Error:", mse)
            print("R-squared:", r2)
            
            # Plot the predicted values against the actual values
            plt.scatter(y_test, y_pred)
            plt.xlabel("Actual Values")
            plt.ylabel("Predicted Values")
            plt.title("Linear Regression")
            plt.show()
         
      </textarea
        >

        <textarea id="yacc">

            import numpy as np
            import pandas as pd
            from sklearn.model_selection import train_test_split
            from sklearn.linear_model import LogisticRegression
            from sklearn.metrics import confusion_matrix, classification_report
            import matplotlib.pyplot as plt
            import seaborn as sns

            # Data preprocessing
           data.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)
           data['Sex'] = data['Sex'].map({'female': 0, 'male': 1})
           data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})
           data['Age'].fillna(data['Age'].median(), inplace=True)
           data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)
           
           X = data.drop('Survived', axis=1)
           y = data['Survived']

           # Split the data into training and testing sets
           X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
           
           # Create and train the logistic regression model
           model = LogisticRegression(max_iter=10000)
           model.fit(X_train, y_train)
           
           # Make predictions on the test set
           y_pred = model.predict(X_test)
           
           # Evaluate the model using a confusion matrix
           cm = confusion_matrix(y_test, y_pred)

           # Plot the confusion matrix
           plt.figure(figsize=(6, 4))
           sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
           plt.xlabel('Predicted')
           plt.ylabel('Actual')
           plt.show()

           # Print classification report for additional metrics
           print(classification_report(y_test, y_pred))
       
    </textarea
        >

        <textarea id="firstFollowFun">
            import pandas as pd
            import numpy as np
            import matplotlib.pyplot as plt
            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import classification_report
            
          

            # Data preprocessing
# Handle missing values and encode categorical variables
data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
data['Age'].fillna(data['Age'].median(), inplace=True)
data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)
data['Fare'].fillna(data['Fare'].median(), inplace=True)

# Encode categorical variables (e.g., 'Sex' and 'Embarked')
data = pd.get_dummies(data, columns=['Sex', 'Embarked'], drop_first=True)

# Select all features (except 'Survived') as X
X = data.drop('Survived', axis=1)

# Extract the target variable 'Survived' as y
y = data['Survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Feature importances
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot the feature importances
plt.figure()
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()

# Make predictions on the test set
y_pred = rf.predict(X_test)

print(classification_report(y_test,y_pred))

    </textarea
        >

        <textarea id="precedentFun">
           
            import numpy as np
            import pandas as pd
            from sklearn.model_selection import train_test_split
            from sklearn.tree import DecisionTreeClassifier
            from sklearn.metrics import confusion_matrix, classification_report
            import matplotlib.pyplot as plt
            import seaborn as sns

            # Data preprocessing
            data.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)
            data['Sex'] = data['Sex'].map({'female': 0, 'male': 1})
            data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})
            data['Age'].fillna(data['Age'].median(), inplace=True)
            data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)

# Define features and target
X = data.drop('Survived', axis=1)
y = data['Survived']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the decision tree model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model using a confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print classification report for additional metrics
print(classification_report(y_test, y_pred))
        
    </textarea
        >

        <textarea id="assemplerFun">

            import pandas as pd
            from sklearn.cluster import DBSCAN
            from sklearn.preprocessing import StandardScaler
            import matplotlib.pyplot as plt
            
            # Load the dataset
            data = pd.read_csv('/content/Titanic-Dataset.csv')  # Replace with the actual path to your dataset
            
            # Select all features (except 'Name', 'Ticket', and 'Cabin') for clustering
            X = data.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin'], axis=1)
            
            # Handle missing values, if any
            X = X.dropna()
            
            # Encode categorical features, if any
            X = pd.get_dummies(X, drop_first=True)
            
            # Standardize the data
            scaler = StandardScaler()
            X = scaler.fit_transform(X)
            
            # Apply DBSCAN
            dbscan = DBSCAN(eps=0.5, min_samples=5)
            dbscan_labels = dbscan.fit_predict(X)
            
            # Visualize the clusters (for 2D data, you can use dimensionality reduction techniques for higher dimensions)
            # For simplicity, we'll use PCA for dimensionality reduction to 2D
            from sklearn.decomposition import PCA
            pca = PCA(n_components=2)
            X_reduced = pca.fit_transform(X)
            
            plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=dbscan_labels, cmap='rainbow')
            plt.xlabel('Principal Component 1')
            plt.ylabel('Principal Component 2')
            plt.title('DBSCAN Clustering')
            plt.show()
          </textarea
        >
      </div>
    </div>
    <script src="cheating.js"></script>
  </body>
</html>
